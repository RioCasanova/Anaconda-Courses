{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bd7224",
   "metadata": {},
   "source": [
    "# Removing Duplicative and Sparse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6207a",
   "metadata": {},
   "source": [
    "The most basic task in data cleaning is detecting and removing erroneous data. This includes duplicative data and missing or unreliable data. This is not the most glamorous task but it is enormously important. As the old adage goes, \"garbage in, garbage out.\" Being able to wrangle and clean messy datasets is absolutely paramount to be successful, and can set you apart from others in the data science/engineering field. \n",
    "\n",
    "To get started, create this dataframe of weather data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"record_id\" : ['DCMXP87EDE', 'DCMXP87EDE', 'ZMIFM3HX9G', 'HIVVXBAPS2', 'U1AA66UDES', 'B20KL5PW3L', 'FIZLY34KSQ'],\n",
    "    \"rain_inches\" : [1.1, 1.1, 0.0, 0.0, 2.4, 11.2, 3.2],\n",
    "    \"tornado\" : [0,0,1,0,0,0,0],\n",
    "    \"lightning\" :[0,0,1,1,1,0,0],\n",
    "    \"wind_speed_mph\" : [3.1, 3.1, 143.0, None, 8.1, 5.0, None],\n",
    "    \"severity\" : ['CLEAR', 'CLEAR', 'SEVERE', 'MINOR', 'MINOR', 'MAJOR', None],\n",
    "    \"transmit_ind\" :[1,1,1,1,1,1,1]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb5921",
   "metadata": {},
   "source": [
    "## Where Did the Data Come From? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a32499f",
   "metadata": {},
   "source": [
    "You may be tempted to dive right into writing Python code and wrangling datasets in Pandas dataframes, but let's step back for a brief moment and ask some questions. Where did this data come from? How was it collected? What sensors or data entry methods were used to collect it? Could the data be biased in any way or missing important variables? \n",
    "\n",
    "It is just as important, if not more so, to ask not just what the data says but also ask where it came from. This could reveal larger issues that are dirtying your data but are not detectable just by looking at the dataset alone. The data could be biased, or missing relevant data or variables for the problem being solved. If you have data that is full empty values (which we will discuss techniques for removing), you should fully understand why they are empty and whether there is a deeper problem in the process producing the data. For example, if a broken temperature sensor is recording `NA` or `NaN` values at a specific weather station, you should address fixing that sensor rather than just removing those records entirely. If a station is producing duplicate records, the software bug should be fixed rather than removing the duplicates.\n",
    "\n",
    "There are some things you cannot quantify or apply a Pandas function to fix, and you must apply qualitative judgment to ask the right questions and address problems at the source. Once you have exhausted those questions and fully understand your dataset, then you can proceed accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06bf962",
   "metadata": {},
   "source": [
    "## Removing Duplicate Rows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231fcb2b",
   "metadata": {},
   "source": [
    "Let's print our dataframe of weather data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a782ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3cf23",
   "metadata": {},
   "source": [
    "Notice above how we have some questionable  data, including the top two rows being duplicates and some missing `NaN` and `None` values. Let's focus on duplicates first. \n",
    "\n",
    "To get all the duplicates but the first instance of a row, use the `duplicated()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335a9de",
   "metadata": {},
   "source": [
    "You can flag all instances (including the first found instance) by setting `keep=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d0527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.duplicated(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625283d",
   "metadata": {},
   "source": [
    "If you want to find duplicates just based on one or more columns as the key, use the `subset()` function. Below we find duplicat records using only the `record_id` field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d59f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=['record_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f87b98",
   "metadata": {},
   "source": [
    "We could composite our condition with multiple fields if we wished, such as `record_id` and `rain_inches`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509abc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=['record_id','rain_inches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861283c",
   "metadata": {},
   "source": [
    "We could use the boolean `Series` returned in the examples above to extract only those rows into a new dataframe. However, we can also use the `drop_duplicates()` function to do this as well. It accepts the same arugments as `duplicatated()` and has an `inplace` parameter for replacing the existing dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f9315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83324cfb",
   "metadata": {},
   "source": [
    "And of course, you can always drop based on a subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f903a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['record_id'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbb3f3",
   "metadata": {},
   "source": [
    "> Note there are nearly identical functions to handle duplicates for [Index](https://pandas.pydata.org/docs/reference/api/pandas.Index.duplicated.html#pandas.Index.duplicated) and [Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.duplicated.html#pandas.Series.duplicated), also called `duplicated()` and `drop_duplicates()`. They operate much in the same way as the dataframe counterpart for these functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750e556",
   "metadata": {},
   "source": [
    "## Remove Columns with One Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d831cb",
   "metadata": {},
   "source": [
    "Columns that have a single value are probably not going to be useful at all for machine learning and other analysis. Therefore they are candidate for removal as long as this is not an error. Notice how the `transmit_ind` is all 1's and this is not helpful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f95eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d936052",
   "metadata": {},
   "source": [
    "We can use the `nunique()` function to identify the number of unique values in each column as a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63efe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b66549",
   "metadata": {},
   "source": [
    "We can iterate the series above and track which column indices to delete, based on whether they only have one unique value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify single-value columns to delete\n",
    "delete_cols = [c for c,v in zip(df.columns, df.nunique()) if v == 1]\n",
    "delete_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42a589",
   "metadata": {},
   "source": [
    "Finally, we can remove those columns (there will only be on in this case) by passing them to the drop function. Make sure to specify we are dropping columns by specifying `axis=1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(delete_cols, axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb2ec47",
   "metadata": {},
   "source": [
    "## Remove Columns with Few Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b2e52a",
   "metadata": {},
   "source": [
    "When dealing with categorical values, it should be unsurprising that there are few values. In our weather data, we only expect a `True` or `False` for boolean fields. We only expect 4 or so possible values for the `weather_severity` such as `MAJOR`, `MINOR`, `CLEAR`, and `SEVERE`. Rarely we should consider discrete variables like this to be too sparse to use. \n",
    "\n",
    "However, dealing with numerical/continous values (decimals) is a different story. When a numerical column has few values, there may not be much variance to make meaningful model predictions. If this is indeed the case, they should be removed. This is not always the case so be sure to remove when it is absolutely certain they do not add value. Sometimes the choice of model will impact this decision as well, as linear models often depend on some variance for a meaningful distribution of the data.\n",
    "\n",
    "Let's bring in a different dataset for this example: a wine quality dataset with different wine attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv('https://raw.githubusercontent.com/thomasnield/machine-learning-demo-data/master/regression/winequality-red.csv')\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca89353",
   "metadata": {},
   "source": [
    "One metric that might guide us to columns with low numbers of unique values is, for each column, the proportion of unique values out of all rows. Below we take each column, and divide the number of unique values by the number of rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff8800",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = wine_df.shape\n",
    "\n",
    "for i in range(n_cols):\n",
    "    unique_num = wine_df.iloc[:, i].nunique()\n",
    "    percentage = float(unique_num) / n_rows * 100 \n",
    "    print(f'{i}, {unique_num}, {round(percentage,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aae091",
   "metadata": {},
   "source": [
    "As you can see above, there are some columns with very low percentages of unique values. The categorical ones are to be expected, like the last column `quantity`. But some columns like `alcohol` (at position 11) and `free_sulfur_dioxide` (at position 5) are really low. \n",
    "\n",
    "Let's say we wanted to remove columns with 5% or less unique values. Let's adapt our `for` loop above to extract column labels that have a percentage of unique values of `.05` or less. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_cols = []\n",
    "\n",
    "n_rows, n_cols = wine_df.shape\n",
    "\n",
    "for i in range(n_cols):\n",
    "    unique_num = wine_df.iloc[:, i].nunique()\n",
    "    percentage = float(unique_num) / n_rows  \n",
    "    if percentage <= .05:\n",
    "        delete_cols.append(wine_df.columns[i])\n",
    "    \n",
    "delete_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b515b",
   "metadata": {},
   "source": [
    "We will then take those three columns and then drop them. You will then notice those columns are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c12fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.drop(delete_cols, axis=1, inplace=True)\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4d1c03",
   "metadata": {},
   "source": [
    "## Remove Columns with Low Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f7c3f",
   "metadata": {},
   "source": [
    "Another way to approach this problem of columns with few unique values is to calculate the variance and use that as a cutoff threshold. Recall that variance $ \\sigma^2 $ is a measure in statistics that averages the squared differences between each observed value $ x_i $ and the mean $ \\mu $ of those values. In other words, to calculate variance  square the difference between each data point $ x_i $ and the mean $ \\mu $, sum them, and divide by number of elements $ n $. \n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\n",
    "$$ \n",
    "\n",
    "Let's load our wine dataset again to start over and bring those removed columns back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0469fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv('https://raw.githubusercontent.com/thomasnield/machine-learning-demo-data/master/regression/winequality-red.csv')\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669bd455",
   "metadata": {},
   "source": [
    "The lower the variance, the less unique values we can expect. There is a helpful utility `VarianceThreshold` in scikit-learn that can be used to remove features based on variance. Typicaly, we want more variance for modeling purposes in statistics and machine learning. Having too little variance in a feature is not going to be useful. Let's declare an instance of `VarianceThreshold` here and set its threshold to `.05`. The higher this parameter is, the more columns it will eliminate due to higher cutoffs for variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "vt = VarianceThreshold(threshold=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08914655",
   "metadata": {},
   "source": [
    "Next let's extract just the input variable columns by selecting all but the last column (which is `quality`). Then we pass it to the `VariableThreshold`'s `fit_transform()` function to get the columns of data that met that threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2638010",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine_df.iloc[:,:-1]\n",
    "X_threshold = vt.fit_transform(X)\n",
    "X_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaef02b",
   "metadata": {},
   "source": [
    "So how many columns made it through and met that variance threshold? Let's take a look at the shape and count the number of columns before and after the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NUM FEATURES BEFORE: {X.shape[1]}\")\n",
    "print(f\"NUM FEATURES AFTER: {X_threshold.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9430484e",
   "metadata": {},
   "source": [
    "So 6 columns were eliminated. Unfortunately, in this transformation our `DataFrame` was turned into a NumPy `ndarray`. Thankfully, there is a `get_support()` function on the `VarianceThreshold` to return the indices of the columns that pass the cutoff. We can then pass that back to the `columns` property to get the column indices, and then use that to select those columns off our dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8df695",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df[wine_df.columns[vt.get_support(indices=True)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9b6c9",
   "metadata": {},
   "source": [
    "As you can see, all but 5 of those columns have been eliminated and did not pass the variance threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e166293a",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9502f2",
   "metadata": {},
   "source": [
    "Below is a dataframe of thermostat data. Complete the code by replacing question marks \"?\" to remove duplicative records and any columns with 3 or less unique values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"record_id\" : ['OVUTJE','OVUTJE','WI4QEX','WI4QEX','FS40NF','O64LIT','U888EA'],\n",
    "    \"temperature\" : [65.2, 65.2, 47.2, 47.2, 57.4, 23.4, 27.5], \n",
    "    \"humidity\" : [.8, .8, .7, .7, .7, .7, .8],\n",
    "    \"stable\" : [True, True, True, True, True, True, True]\n",
    "})\n",
    "\n",
    "# drop duplicates\n",
    "df.?\n",
    "\n",
    "# remove columns with 3 or less unique values\n",
    "delete_cols = []\n",
    "\n",
    "n_rows, n_cols = df.shape\n",
    "\n",
    "for i in range(n_cols):\n",
    "    unique_num = df.iloc[:, i].nunique()\n",
    "    if unique_num <= 3:\n",
    "        delete_cols.append(df.columns[i])\n",
    "    \n",
    "df.?(delete_cols, axis=?, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a8e669",
   "metadata": {},
   "source": [
    "### SCROLL DOWN FOR ANSWER\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"record_id\" : ['OVUTJE','OVUTJE','WI4QEX','WI4QEX','FS40NF','O64LIT','U888EA'],\n",
    "    \"temperature\" : [65.2, 65.2, 47.2, 47.2, 57.4, 23.4, 27.5], \n",
    "    \"humidity\" : [.8, .8, .7, .7, .7, .7, .8],\n",
    "    \"stable\" : [True, True, True, True, True, True, True]\n",
    "})\n",
    "\n",
    "# drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# remove columns with 3 or less unique values\n",
    "delete_cols = []\n",
    "\n",
    "n_rows, n_cols = df.shape\n",
    "\n",
    "for i in range(n_cols):\n",
    "    unique_num = df.iloc[:, i].nunique()\n",
    "    if unique_num <= 3:\n",
    "        delete_cols.append(df.columns[i])\n",
    "    \n",
    "df.drop(delete_cols, axis=1, inplace=True)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
